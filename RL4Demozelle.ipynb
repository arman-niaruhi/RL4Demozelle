{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project RL4Demozelle\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Communication to MQTT-Broker via Paho\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paho.mqtt.client as mqtt\n",
    "import ssl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MqttClient():\n",
    "    def __init__(self, username, password, server_address, port, topics):\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.server_address = server_address\n",
    "        self.port = port\n",
    "        self.topics = topics\n",
    "        self.last_received_values = {}  # Dictionary to store values of every topic\n",
    "        self.message_received = False\n",
    "\n",
    "        # initialize mqtt client\n",
    "        self.client = mqtt.Client()\n",
    "        self.client.on_connect = self.on_connect\n",
    "        self.client.on_message = self.on_message\n",
    "\n",
    "        # enable tls for secure connection\n",
    "        self.client.tls_set(tls_version=ssl.PROTOCOL_TLS)\n",
    "\n",
    "        # set username and password\n",
    "        self.client.username_pw_set(self.username, self.password)\n",
    "\n",
    "        # connect to the broker\n",
    "        self.client.connect(self.server_address, self.port, keepalive=60)\n",
    "\n",
    "        # subscribe to the topics\n",
    "        for topic in self.topics:\n",
    "            self.client.subscribe(topic)\n",
    "            # Initialisierung f√ºr jedes Topic\n",
    "            self.last_received_values[topic] = None\n",
    "\n",
    "        # start loop to wait for messages\n",
    "        self.client.loop_start()\n",
    "\n",
    "    # The callback for when the client receives a CONNACK response from the server.\n",
    "    def on_connect(self, client, userdata, flags, rc):\n",
    "        print(\"Connected with result code \" + str(rc))\n",
    "\n",
    "    # The callback for when a PUBLISH message is received from the server.\n",
    "    def on_message(self, client, userdata, msg):\n",
    "        self.message_received = True\n",
    "        received_value = float(msg.payload.decode('utf-8').replace(',', '.'))\n",
    "        topic = msg.topic\n",
    "        # print(f'Received Value for Topic {topic}: {received_value}')\n",
    "        self.last_received_values[topic] = received_value\n",
    "        # print(self.last_received_values[topic])\n",
    "\n",
    "        return self.last_received_values[topic]\n",
    "\n",
    "    def publish(self, topic, value, qos=0, retain=False):\n",
    "        # publish value for a specific topic\n",
    "        self.client.publish(topic, payload=float(value),\n",
    "                            qos=qos, retain=retain)\n",
    "        # print(f\"Published values on topic {topic}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = 'UiN7dRqkM4ZA'\n",
    "PASSWORD = 'YGmSKvebsz7V..V'\n",
    "SERVER_ADDRESS = '3c4fdc3e60c54d06b6252a909b39100e.s2.eu.hivemq.cloud'\n",
    "SERVER_PORT = 8883\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RL Environment via OpenAi Gym library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from scipy.interpolate import CubicSpline\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemocellEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        # Definition of inital speed values\n",
    "        self._conveyor_speed_1 = 0\n",
    "        self._conveyor_speed_2 = 0\n",
    "\n",
    "        # Definition of the speed and centricity threshhold\n",
    "        self._conveyor_speed_1_interval = (300, 1100)\n",
    "        self._conveyor_speed_2_interval = (300, 1100)\n",
    "        self._centrcity_threshold = 0.25\n",
    "\n",
    "        # List of rewards\n",
    "        self.rewards = []\n",
    "\n",
    "        # List of time per steps\n",
    "        self.list_of_time_per_step = []\n",
    "\n",
    "        # MQTT-Server credentials\n",
    "        self.username = USERNAME\n",
    "        self.password = PASSWORD\n",
    "        self.server_address = SERVER_ADDRESS\n",
    "        self.server_port = SERVER_PORT\n",
    "\n",
    "        # MQTT Topics for subscribing\n",
    "        self.subscriber_topics = ['Democell/Quality/Centricity']\n",
    "\n",
    "        # Initialize of the mqtt client\n",
    "        self.mqtt_client = MqttClient(username=self.username, password=self.password,\n",
    "                                      server_address=self.server_address, port=self.server_port, topics=self.subscriber_topics)\n",
    "\n",
    "        # Definition of the action space\n",
    "        # Action space with 2 discrete speed values for each conveyor (increase or decrease speed); in total 4 dicrete values\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "\n",
    "        # Definition of the observation space\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "            # Centricity value in an interval between 0.0 and 1.0, with 0.0 is not centered at all and 1.0 is perfectly centered\n",
    "            'centricity': gym.spaces.Box(low=0, high=1, shape=(1,), dtype=float),\n",
    "            # Speed values for conveyor speed 1 between 150 and 1100\n",
    "            'conveyor_speed_1': gym.spaces.Box(low=self._conveyor_speed_1_interval[0], high=self._conveyor_speed_1_interval[1], shape=(1,), dtype=float),\n",
    "            # Speed values for conveyor speed 2 between 150 and 1100\n",
    "            'conveyor_speed_2': gym.spaces.Box(low=self._conveyor_speed_2_interval[0], high=self._conveyor_speed_2_interval[1], shape=(1,), dtype=float),\n",
    "        })\n",
    "\n",
    "        '''\n",
    "        The following dictionay maps the abstract actions from 'self.action_space' to the speed action it will take.\n",
    "        So that 0 corresponds to 'increase' conveyor speed, whereas 1 corresponds to 'decrease' conveyor speed.\n",
    "        0 & 1: Conveyor speed 1 will be changed\n",
    "        2 & 3: Conveyor speed 2 will be changed\n",
    "        '''\n",
    "        self._action_to_speed_change = {\n",
    "            0: float(-50),  # Decrease conveyor_speed_1\n",
    "            1: float(50),   # Increase conveyor_speed_1\n",
    "            2: float(-50),  # Decrease conveyor_speed_2\n",
    "            3: float(50)    # Increase conveyor_speed_2\n",
    "        }\n",
    "\n",
    "    # Returns an array of speed values for _conveyor_speed_1, _conveyor_speed_2 and centricity\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            'conveyor_speed_1': np.array([self._conveyor_speed_1], dtype=float),\n",
    "            'conveyor_speed_2': np.array([self._conveyor_speed_2], dtype=float),\n",
    "            'centricity': np.array([self._centricity], dtype=float)\n",
    "        }\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            'conveyor_speed_1': self._conveyor_speed_1,\n",
    "            'conveyor_speed_2': self._conveyor_speed_2,\n",
    "            'centricity': self._centricity\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        '''\n",
    "        Resets the values for Conveyor_speed_1 to a random number between the max speed value and min speed value from conveyor_speed_1_interval\n",
    "        '''\n",
    "        # Inherit from seed.np_random to set seed\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Reset current_step and centricity value\n",
    "        self.current_step = 0\n",
    "        # self._centricity = self.mqtt_client.last_received_values['Democell/Quality/Centricity']\n",
    "        self._centricity = None\n",
    "\n",
    "        # Set speed values for conveyor 1 & 2 to random speed between minimal and maximal speed and publish the values to mqtt\n",
    "        self._conveyor_speed_1 = self.np_random.uniform(\n",
    "            low=self._conveyor_speed_1_interval[0], high=self._conveyor_speed_1_interval[1], size=1).astype(float)\n",
    "        self._conveyor_speed_1 = self._conveyor_speed_1[0]\n",
    "        self._conveyor_speed_2 = self.np_random.uniform(\n",
    "            low=self._conveyor_speed_2_interval[0], high=self._conveyor_speed_2_interval[1], size=1).astype(float)\n",
    "        self._conveyor_speed_2 = self._conveyor_speed_2[0]\n",
    "\n",
    "        # Get back the observation parameters and additional infos\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        '''\n",
    "        For the step the action from the action space is mapped. For action 0 in this step the speed will be reduced by 0.01 for action 1 the speed will be increased by 0.01.\n",
    "        Also after setting the speed to an increased or decreased speed, this function will wait for some seconds and read out the corresponding centricity vlaue.\n",
    "        After getting the centrcity value the function will then calculate a reward based on the speed and centricty values.\n",
    "        '''\n",
    "\n",
    "        # Start the step\n",
    "        start_time = time.time()\n",
    "        # Map the abstract action to the change in speed\n",
    "        speed_change = self._action_to_speed_change[action]\n",
    "\n",
    "        # Execute action depending on the Discrete number of the action space: for 0 & 1 Conveyor Speed 1 will be changed, for 3 & 4 Conveyor Speed 2 will be changed\n",
    "        # # Use 'np.clip' to make sure we do not leave the speed intervals\n",
    "        action_list = int(action)\n",
    "        if action_list in [0, 1]:  # Actions 0 and 1 correspond to conveyor_speed_1\n",
    "            # self._conveyor_speed_1 = float(np.clip(self._conveyor_speed_1 + speed_change, self._conveyor_speed_1_interval[0], self._conveyor_speed_1_interval[1]))\n",
    "            self._conveyor_speed_1 += speed_change\n",
    "        else:  # Actions 2 and 3 correspond to conveyor_speed_2\n",
    "            # self._conveyor_speed_2 = float(np.clip(self._conveyor_speed_2 + speed_change, self._conveyor_speed_2_interval[0], self._conveyor_speed_2_interval[1]))\n",
    "            self._conveyor_speed_2 += speed_change\n",
    "\n",
    "        # Convert speed values to speed values in m/s\n",
    "        _conveyor_speed_1_converted = self.calc_speed_value_to_ms(\n",
    "            self._conveyor_speed_1)\n",
    "        print(f'Conveyor Speed 1: {self._conveyor_speed_1}')\n",
    "        _conveyor_speed_2_converted = self.calc_speed_value_to_ms(\n",
    "            self._conveyor_speed_2)\n",
    "        print(f'Conveyor Speed 2: {self._conveyor_speed_2}')\n",
    "\n",
    "        # Publish new speed values to the mqtt server\n",
    "        self.mqtt_client.publish('Democell/Speed/Conveyor_Speed_1',\n",
    "                                 value=_conveyor_speed_1_converted, qos=0, retain=False)\n",
    "        self.mqtt_client.publish('Democell/Speed/Conveyor_Speed_2',\n",
    "                                 value=_conveyor_speed_2_converted, qos=0, retain=False)\n",
    "\n",
    "        # Wait for new centricity value\n",
    "        self.mqtt_client.client.loop_start()\n",
    "        self.mqtt_client.message_received = False\n",
    "        while not self.mqtt_client.message_received:\n",
    "            sleep(2)\n",
    "            if self.mqtt_client.message_received == True:\n",
    "                self.mqtt_client.message_received = False\n",
    "                break\n",
    "\n",
    "        self._centricity = self.mqtt_client.last_received_values['Democell/Quality/Centricity']\n",
    "        print(f'Centricity: {self._centricity}')\n",
    "        print('_________________________________________________________')\n",
    "\n",
    "        reward = self.calculate_reward(\n",
    "            self._centricity, self._conveyor_speed_1, self._conveyor_speed_2)\n",
    "\n",
    "        self.rewards.append(round(reward, 3))\n",
    "\n",
    "        # Episode terminates after 10 steps\n",
    "        self.max_steps_per_episode = 2\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps_per_episode:\n",
    "            terminated = True\n",
    "        else:\n",
    "            terminated = False\n",
    "        # Calculate the time of each step\n",
    "        time_of_step = time.time() - start_time\n",
    "        self.list_of_time_per_step.append(time_of_step)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "\n",
    "    def calculate_reward(self, centricity, speed_1, speed_2):\n",
    "        # Define the reward function; For centricity values above threshold give strong negative reward\n",
    "        if centricity > self._centrcity_threshold:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = speed_1 + speed_2\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def calc_speed_value_to_ms(self, speed_value):\n",
    "        # Datapoint from Excel 'Bandgeschwindigkeiten_Demozelle', Column: eingestellte Geschwindigkeit [1]; Column: Bandgeschwindigkeit [m/s]\n",
    "        geschwindigkeiten = np.array([150, 200, 250, 300, 350, 400, 450, 500,\n",
    "                                     550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100])\n",
    "        bandgeschwindigkeiten = np.array([0.021, 0.039, 0.058, 0.076, 0.093, 0.114, 0.131, 0.148,\n",
    "                                         0.165, 0.184, 0.203, 0.218, 0.236, 0.262, 0.276, 0.293, 0.308, 0.321, 0.323, 0.328])\n",
    "\n",
    "        # Calculate cubic function\n",
    "        interpolation = CubicSpline(geschwindigkeiten, bandgeschwindigkeiten)\n",
    "\n",
    "        # Interpolation with cubic-function for speed values to metre/sec\n",
    "        result = interpolation(speed_value)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conveyor Speed 1: 537.646292788247\n",
      "Conveyor Speed 2: 753.0315260910389\n",
      "Connected with result code 0\n",
      "Centricity: 0.05340784\n",
      "_________________________________________________________\n",
      "Conveyor Speed 1: 587.646292788247\n",
      "Conveyor Speed 2: 753.0315260910389\n",
      "Centricity: 0.04110626\n",
      "_________________________________________________________\n",
      "Conveyor Speed 1: 273.8546851974731\n",
      "Conveyor Speed 2: 782.9846958676707\n",
      "Centricity: 0.05299375\n",
      "_________________________________________________________\n",
      "Conveyor Speed 1: 273.8546851974731\n",
      "Conveyor Speed 2: 832.9846958676707\n",
      "Centricity: 0.03267279\n",
      "_________________________________________________________\n",
      "start\n",
      "Conveyor Speed 1: 869.1648388447707\n",
      "Conveyor Speed 2: 651.1027518016418\n",
      "Centricity: 0.06484357\n",
      "_________________________________________________________\n",
      "start\n",
      "Conveyor Speed 1: 919.1648388447707\n",
      "Conveyor Speed 2: 651.1027518016418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 16:53:50 [DEBUG] top of Axes not in the figure, so title not moved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centricity: 0.04603009\n",
      "_________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 16:53:50 [DEBUG] top of Axes not in the figure, so title not moved\n",
      "2024-01-31 16:53:50 [DEBUG] top of Axes not in the figure, so title not moved\n",
      "2024-01-31 16:53:50 [DEBUG] top of Axes not in the figure, so title not moved\n",
      "2024-01-31 16:53:51 [WARNING] Learning Rate : 0.01, BUFFER_SIZE : 0, Learning_starts:0, batch size: 0\n",
      "2024-01-31 16:53:51 [INFO] Episode: 2, Step: 2, Reward: 1570.2675906464124, Speed 1: 919.1648388447707, Speed 2: 651.1027518016418, Centricity: 0.04603009 \n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode terminated. Exiting the program.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiate custom gym RL environment\n",
    "env = DemocellEnv()\n",
    "\n",
    "# Hyperparameters fpr DQN-Agent\n",
    "LR = 1e-2\n",
    "BUFFER_SIZE = 0\n",
    "LEARNING_STARTS = 0\n",
    "BATCH_SIZE = 0\n",
    "GAMMA = 0.95\n",
    "\n",
    "# Instantiate stablebaselines3 DQN Agent; As policy 'MultiInputPolicy' is needed sind we use a dictionary as observation space\n",
    "DQNAgent = DQN(policy='MultiInputPolicy', env=env, verbose=0, learning_rate=LR,\n",
    "               learning_starts=LEARNING_STARTS, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, gamma=GAMMA)\n",
    "# Learning with the DQN-Agent\n",
    "DQNAgent.learn(total_timesteps=2, log_interval=10)\n",
    "DQNAgent.set_random_seed(42)\n",
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "# Used for logging information about each episode; it will debu\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "logger = logging.getLogger(__name__)\n",
    "# Create a FileHandler to save log info in a text file\n",
    "file_handler = logging.FileHandler('result.txt', mode='a')\n",
    "file_handler.setLevel(logging.DEBUG)  # Set the level as desired\n",
    "\n",
    "logger.addHandler(file_handler)\n",
    "eps_counter = 0\n",
    "step_counter = 0\n",
    "counter = None\n",
    "# Loop for teaching the model; here it will explore and exploit the gym custom environment\n",
    "while True:\n",
    "    print(\"start\")\n",
    "    action, _states = DQNAgent.predict(obs, deterministic=True)\n",
    "    action = int(action)\n",
    "    obs, reward, terminated, test, info = env.step(action)\n",
    "\n",
    "    # Log information to the notebook; it will also count the episodes and the used steps of each episode\n",
    "    eps_counter += 1\n",
    "    step_counter += 1\n",
    "    plt.clf()\n",
    "    # If Terminated, exit the loop and end the training\n",
    "    if terminated:\n",
    "       # Extract episode rewards from the Monitor\n",
    "        episode_rewards = env.rewards\n",
    "        duration_for_steps = env.list_of_time_per_step\n",
    "\n",
    "        # Create a figure with two subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(\n",
    "            10, 8), sharex=True, facecolor='white')\n",
    "\n",
    "        # Plot Total Rewards on the first subplot\n",
    "        color = 'tab:blue'\n",
    "        ax1.set_ylabel('Total Reward', color=color)\n",
    "        ax1.plot(episode_rewards, color=color, label='Total Reward')\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        # Plot Duration for Each Step on the second subplot\n",
    "        color = 'tab:orange'\n",
    "        ax2.set_xlabel('Steps')\n",
    "        ax2.set_ylabel('Duration', color=color)\n",
    "        ax2.plot(duration_for_steps, color=color, linestyle='--',\n",
    "                 label='Time steps for each step')\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        # Title and legends\n",
    "        fig.suptitle('Total Reward and Duration for Each Step')\n",
    "        # Adjust the layout to avoid title overlap\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        fig.legend(loc='upper right')\n",
    "\n",
    "        # Save the plot\n",
    "        plt.savefig(\n",
    "            f'Reward_and_Duration for episode {eps_counter}.png', facecolor='white')\n",
    "        plt.clf()\n",
    "\n",
    "        # Example usage of the logger\n",
    "        if (counter is None): \n",
    "            logger.warning(\n",
    "                f'Learning Rate : {LR}, BUFFER_SIZE : {BUFFER_SIZE}, Learning_starts:{LEARNING_STARTS}, batch size: {BATCH_SIZE}')\n",
    "            logger.info(\n",
    "                f\"Episode: {eps_counter}, Step: {step_counter}, Reward: {reward}, Speed 1: {info['conveyor_speed_1']}, Speed 2: {info['conveyor_speed_2']}, Centricity: {info['centricity']} \\n\" + 60*\"--\")\n",
    "            counter = 1\n",
    "\n",
    "        obs, info = env.reset()\n",
    "        print(\"Episode terminated. Exiting the program.\")\n",
    "        break\n",
    "# Save model to the same directory\n",
    "DQNAgent.save('DQN_Democell')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RL Agent via PyTorch\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration of matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x260ef5c3dd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        # Save a transition\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...])s.\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters and utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published values on topic Democell/Conveyor_Speed_1: 0.3935546875\n",
      "Published values on topic Democell/Conveyor_Speed_1: 0.464599609375\n"
     ]
    }
   ],
   "source": [
    "env = DemocellEnv()\n",
    "env.reset()\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Get number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected with result code 0\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                       if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(\n",
    "            non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values,\n",
    "                     expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published values on topic Democell/Conveyor_Speed_1: 0.63427734375\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Initialize the environment and get it's state\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     state, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m----> 9\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m count():\n\u001b[0;32m     11\u001b[0m         action \u001b[38;5;241m=\u001b[39m select_action(state)\n",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not dict"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get it's state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32,\n",
    "                         device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(\n",
    "                observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # Œ∏‚Ä≤ ‚Üê œÑ Œ∏ + (1 ‚àíœÑ )Œ∏‚Ä≤\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key] * \\\n",
    "                TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
